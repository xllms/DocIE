<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Document-level Infroamtion Extraction.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<!--     <title>XLLM ACL 2025 Shared Task-II: Speech Event Extraction</title> -->
    <title> XLLM ACL 2025 Shared Task-IV: Document-level Information Extraction </title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <!--  <script src="./static/js/index.js"></script>-->
    <style>
        pre {outline: 1px solid #ccc; }
         .string { color: green; }
         .number { color: darkorange; }
         .boolean { color: blue; }
         .null { color: magenta; }
         .key { color: red; }
        ._table{width: 100%; border-collapse: collapse; border:0px;}
        ._table thead tr {font-size: 13px; color: #2e3b45;  text-align: center; background-color: rgba(230, 255, 250, 0.92); font-weight:bold;}
        ._table td{line-height: 20px; text-align: center; padding: 4px 10px 3px 10px; height: 18px;border: 0px solid #ffffff;}
        ._table tbody tr {background: #fff; font-size: 13px; color: #393939;}
        ._table tbody tr:nth-child(2n){ background: #f3f3f3;}
    </style>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"><a href="https://xllms.github.io/">XLLM</a> @ ACL 2025 Shared Task-IV:</h1>
                        <h1 class="title is-1 publication-title">Universal Document-level Information Extraction (<a href="#">DocIE</a>)</h1> <!--Add Link Here-->
                        

                        <div class="column has-text-centered">
                            <div class="publication-links">

                                                            
                                <!-- Data Link. -->
                                <span class="link-block">
                                    <a href="#introduction"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Introduction</span>
                                    </a>
                                </span>
                                
                                <!-- Data Link. -->
                                <span class="link-block">
                                    <a href="#dataset"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Dataset</span>
                                    </a>
                                </span>
                                <!-- Paper Link. -->
                                <span class="link-block">
                                    <a href="#task"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Task & Evaluation</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->

                                
                                <span class="link-block">
                                    <!--Add Link Here-->
                                    <a href="#" 
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Code</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="#submission"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Submission</span>
                                    </a>
                                </span>

                                

                                <span class="link-block">
                                    <a href="#timeline"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Timeline</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <!--Add Link Here-->
                                    <a href="https://www.codabench.org/competitions/6641/"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>CodaBench</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <section class="section"style="margin-top: -50px;">
        <div class="container is-max-desktop">

            <!-- <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Leaderboard</h2>
                    <div class="content has-text-justified">
                        <p>
                            Congratulations to the winners!
                        </p>
                        <table class="._table table-bordered table-striped" align="center">
                            <tbody align="center" valign="center">
                            <tr>
                                <td>Rank</td>
                                <td>Team</td>
                                <td>Score</td>
                              </tr>
                            <tr>
                              <td>1</td>
                              <td>Token</td>
                              <td>63.2064</td>
                            </tr>
                            <tr>
                              <td>2</td>
                              <td>USTC-IAT-United</td>
                              <td>62.1149</td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>ppjj</td>
                                <td>59.8638</td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td>GXU-LIPE</td>
                                <td>59.6864</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>DMCV</td>
                                <td>59.3998</td>
                            </tr>
                            
                        </tbody>
                        </table>
                    </div>
                </div>
            </div> -->
            
            <!-- Abstract. -->
            <div id="introduction" class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Introduction</h2>
                    <div class="content has-text-justified">

                        <div style="text-align: center;">
                            <img src="static/images/overview_new.pdf" width="90%" alt="">
                            <p><b>Overview: Document-level Information Extraction Task.</b></p>
                        </div>

                        <p>
                        We introduce the Document-level Information Extraction (DocIE) challenge on this platform. The goal of DocIE is to identify entities, their corresponding mentions, and the relationships between entity pairs within long, unstructured documents. This challenge requires models to process an input document, which consists of a sequence of sentences, and produce output structures that include three key elements: 1) sets of mentions, where each set corresponds to a distinct entity; 2) entity types; and 3) relation triples, which describe the relationships between pairs of entities.
                        </p>
                        <p>
                            You're welcome to join our <a href="https://join.slack.com/t/xllmdocumentl-mh69179/shared_invite/zt-318mjl0kk-eMuhjBYSIg2b8T1Lb_n8XQ" >Slack community </a>—feel free to ask questions and connect with us!
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
             <!-- Datasets. -->
             <div class="columns is-centered has-text-centered">
                <div id="dataset" class="column is-four-fifths">
                    <h2 class="title is-3">Dataset</h2>
                    <div class="content has-text-justified">

                        <p>
                            The distribution of domains and source datasets within the DocIE, including 34 domains datasets, is shown in the figure above. The DocIE is a comprehensive dataset that covers a wide range of domains and source datasets, making it a valuable resource for researchers and practitioners in the field of Document-level information extraction.

                        <div style="text-align: center;">
                            <img src="static/images/statistic.png" width="90%" alt="">
                            <p><b>Distribution of different tasks, domains, and source datasets within the DocIE.</b></p>
                        </div>
                        </p>


                        <p>
                            Example of a sample:
                        </p>
                        <pre id="jsonShow">
{
    "domain": "Culture",
    "title": "The_Triple_Package",
    "doc": "The Triple Package: How Three Unlikely Traits Explain the Rise and Fall of Cultural Groups in America is a book published in 2014 by two professors at Yale Law School, Amy Chua and her husband, Jed Rubenfeld. Amy Chua is also the author of the 2011 international bestseller, Battle Hymn of the Tiger Mother.\nAccording to the preface, the authors find that \"certain groups do much better in America than others\u2014as measured by various socioeconomic indicators such as income, occupational status, job prestige, test scores, and so on\u2014 [which] is difficult to talk about. In large part this is because the topic feels racially charged.\" Nevertheless, the book attempts to debunk racial stereotypes by focusing on three \"cultural traits\" that attribute to success in the United States.\nFollowing Battle Hymn of the Tiger Mother in 2011, Chua wrote this book with her husband Jed Rubenfeld after observing a more prevalent trend of students from specific ethnic groups achieving better academic results than other ethnic groups. For example, ..........",
    "triplets": [
        {
            "subject": "The Triple Package",
            "relation": "ReviewedBy",
            "object": "Colin Woodard"
        },
        {
            "subject": "The Triple Package",
            "relation": "Creator",
            "object": "Jed Rubenfeld"
        },
        ... ...
    ],
    "entities":[ 
        {
            "id": 0,
            "mentions": [
                "the United States",
                "America",
                "U.S.",
                "American",
                "the successful groups in the United States",
                "the rest in America",
                "the national average",
                "the American dream",
                "UK"
            ],
            "type": "GPE"
        },
        {
            "id": 1,
            "mentions": [
                "Yale Law School",
                "Yale"
            ],
            "type": "ORG"
        },
        ... ...
    ],
    "label_set": ["ReviewedBy","NominatedFor","InfluencedBy","AwardReceived","HasWorksInTheCollection","Creator","PresentedIn","EthnicGroup","PublishedIn","Affiliation","OwnerOf","InvestigatedBy","CitesWork","HasPart","Acknowledged","DifferentFrom","Follows"],
    "entity_label_set":['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']
}
</pre>
                        <p>
                            <!--Add Link Here-->
                            The datasets are avaliable on <a href="https://drive.google.com/drive/folders/1ll2feQBdds2uOdoaQ-j4f4HIE63cOdUi?usp=drive_link">Google Drive</a>.
                        </p>
                    </div>
                </div>
            </div>

            <!-- Paper video. -->
            <div id="task" div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Challenge Task Definition and Metrtics</h2>
                    <div class="content has-text-justified">
                        <div style="text-align: center;">
                            <img src="static/images/task1.pdf" width="90%" alt="">
                            <p><b>Task 1: Named Entity Recognition.</b></p>
                        </div>
                        <p>
                            <b>Task-1:</b> Named Entity Recognition (NER) involves identifying named entities within a given text and classifying them into appropriate categories. Participants are expected to develop models that accurately extract both the entities mand their corresponding types. Unlike traditional sentence-level NER tasks, this task requires participants to identify all mentions of each entity within the entire paragraph.
                        </p>
                        
                        <!-- <div style="text-align: center;">
                            <img src="static/images/task2.pdf" width="90%" alt="">
                            <p><b>Task 2: Coreference.</b></p>
                        </div>
                        <p>
                            <b>Task-2:</b>
                            Coreference aims to identify the coreference relations between two or more mentions in the given paragraph. Participants are required to develop models that can accurately identify the coreference relations and their corresponding types. A coreference relation is considered correctly identified if it matches the gold standard relation and its corresponding type. The evaluation metric for this sub-task is the F1 score.
                         <p> -->
                        
                        <div style="text-align: center;">
                            <img src="static/images/task2_new.pdf" width="90%" alt="">
                            <p><b>Task 2:  Relation Extraction.</b></p>
                        </div>
                        <p>
                            <b>Task-2:</b> Relation Extraction(RE) involves identifying the relations between entities within a given text. Participants are expected to develop models that accurately extract both entitiy pairs and its relaion types. Unlike traditional sentence-level RE tasks, this task requires participants to identify all relations of each entity pair within the entire paragraph.
                        </p>


                        
                        <div class="columns is-centered has-text-centered">
                            <div>
                                <h2 class="title is-3">Evaluation</h2>
                                <div class="content has-text-justified">
                                    <h3> Basic Metric</h3>
                                    The F1 score is an important evaluation metric for information extraction tasks, which is calculated by Precision \(P\)  and Recall \(R\):
                                         <p>$$ P = \frac{TP}{ TP + FP } $$ </p>
                                         <p>$$ R = \frac{TP}{TP + FN} $$ </p>
                                         <p>$$ F1 = \frac{2 \times P \times R }{ P + R } $$ </p>
                                         <ul>
                                             <li>\(TP\): True Positive ratio, which indicates the number of samples that were correctly predicted by the model.</li>
                                             <li>\(FP\): False Positive ratio, which indicates the number of samples that were incorrectly predicted by the model.</li>
                                             <li>\(FN\): False Negative ratio, which the number of samples that the model's prediction are inconsistent with Ground True .</li>
                                         </ul>
                                         <p>We will use the F1 score as the main evaluation metric. However, for different settings (strict mode and general mode), there are different criteria for determining whether a sample is correctly predicted, which we will introduce in detail.
                                        </p>
                                         
                                        <!--我们将使用F1分数作为主要的评估指标。然而，对于不同的设置（严格模式和一般模式）下，判断一个样本是否被正确预测存在不同的判定标准，我们将进行详细地介绍： -->

                                    <h3> Named Entity Recognition</h3>
                                     <p>The named entity recognition task can be divided into Entity Identification and Entity Type Classification at a more fine-grained level. In order to evaluate the capabilities of the model more systematically, we set different evaluation indicators for these two aspects: </p>
                                     <p><div style="text-align: center;">
                                        <img src="static/images/NER.pdf" width="90%" alt="">
                                        <p><b>Example of named entity recognition task. When an entity mention set is judged to be predicted correctly, the value of \(TP_{\text{EI}}\) or \(TP_{\text{EC}}\) increases by 1. Therefore, the final value of \(TP_{\text{EI}}\) or \(TP_{\text{EC}}\) represents the number of correctly extracted entity.</b>
                                        </p>
                                    </div></p>
                                     <h5>Entity Identification</h5>
                                     <p>Participants are required to correctly extract all the entity mentions from the given text. Only all predicted mentions match the ground truth, the sample will be considered correctly predicted. And the value of \(TP_{\text{EI}} \)  is the number of samples predicted correctly. Finally, according to the calculation formula of F1 score, \(F1_{EI}\) can be calculated as the evaluation metric of this subtask.</p>
                                     

                                     <h5>Entity Classification</h5>
                                    <p>
                                        Participants are required to classify all predicted mentions into the correct entity types. Only all mentions are classify correctly, the sample will be considered correctly predicted. And the value of \(TP_{\text{EC}} \)  is the number of samples classify correctly. Finally, according to the calculation formula of F1 score, \(F1_{EC}\) can be calculated as the evaluation metric of this subtask.</p>

                                    </p>

                                    

                                    <h3> Relation Extraction</h3>
                                        <p>The evaluation of the relation extraction will be divided into two different settings: general mode and strict mode.</p>
                                        <div style="text-align: center;">
                                            <img src="static/images/RE.pdf" width="90%" alt="">
                                            <p><b>Example of relation extraction task in general mode and strict mode. When a triple is judged to be predicted correctly, the value of \(TP_{\text{REG}}\) increases by 1. Therefore, the final value of \(TP_{\text{REG}}\) represents the number of correctly predicted triplets.</b></p>
                                        </div>


                                        <ul>
                                            <li> <b>General Mode</b>: Participants require to accurately extract all entity mentions and their relations in a given text, i.e. accurately predict the relation triplets (head entity mention, relation, tail entity mention). If the head entity mention and tail entity mention are replaced by another mention in the same mention set, we still consider the sample was predicted correctly. \(TP_{\text{REG}}\) is the number of triples predicted correctly in general mode. And based on the above discrimination criteria and the basic metric, \(F1_{REG}\) can be calculated as the evaluation metric of this subtask.</li>

                                            <li> <b>Strict Mode</b>: Participants require to accurately extract all entities mentions and their relations. In this setting, we strictly compare the difference between the ground true and the predicted results. Therefore, if the head entity mention and tail entity mention are replaced by another mention in the same mention set, we will consider the sample was <b>not</b> predicted correctly. \(TP_{\text{RES}}\) is the number of triples predicted correctly in strict mode. And based on the above discrimination criteria and the basic metric, \(F1_{RES}\) can be calculated as the evaluation metric of this subtask</li>
                                        </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            



 

            <!-- / Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div id="code" class="column is-four-fifths">
                    <h2 class="title is-3">Baseline</h2>
                    <div class="content has-text-justified">
                      
                        <p>
                            <!--Add Link Here-->
                            Baseline code and full data will publish as soon as possible.
                        </p>
                        <!-- <p style="color: red;">
                            <b>The final test set evaluation will be available on March 20th in CodeBench. Participants can currently self-evaluate the provided training or development sets using the evaluation script we’ve provided. Final results will be calculated using the same evaluation methods.</b>
                        </p> -->
                        <p style="color: red;">
                            The evaluation code is available on <a href="https://github.com/xllms/DocIE/blob/main/scoring.py">Github</a>. Participants need to specify the paths for the prediction results file (results.json), the GroundTruth file (reference.json), and the result saving path (scores.json) in the evaluation script (scoring.py), and then run the evaluation script using the following command:
                        </p>
                        <pre>
                            <code>
python scoring.py</code>
                        </pre>



                        <p>
                            We utilize GPT-4o and llama3-8b-all as the baseline models for the DocIE challenge. Their performance is shown in the table below, and we have released the prediction code for GPT-4o as the baseline code(<a href="https://github.com/xllms/DocIE/tree/main/baseline_code_gpt4o"> baseline_code_gpt4o/gpt4o.py </a>).
                        </p>

                        <table>
                            <tr>
                                <th rowspan="2">Category</th>
                                <th colspan="3">gpt4o</th>
                                <th colspan="3">llama3-8b-all</th>
                            </tr>
                            <tr>
                                <th>F1</th><th>P</th><th>R</th>
                                <th>F1</th><th>P</th><th>R</th>
                            </tr>
                            <tr><td>Academic_disciplines</td><td>3.25</td><td>2.78</td><td>3.9</td><td>5.07</td><td>7.23</td><td>3.9</td></tr>
                            <tr><td>Business</td><td>1.41</td><td>1.71</td><td>1.2</td><td>4.08</td><td>13.33</td><td>2.41</td></tr>
                            <tr><td>Communication</td><td>10.26</td><td>13.73</td><td>8.19</td><td>2.76</td><td>6.25</td><td>1.75</td></tr>
                            <tr><td>Culture</td><td>7.31</td><td>9.4</td><td>5.98</td><td>5.38</td><td>8.91</td><td>3.85</td></tr>
                            <tr><td>Economy</td><td>2.6</td><td>2.87</td><td>2.37</td><td>7.17</td><td>14.71</td><td>4.74</td></tr>

                            <tr><td>Education</td><td>1.21</td><td>1.28</td><td>1.15</td><td>2.59</td><td>2.96</td><td>2.3</td></tr>
                            <tr><td>Energy</td><td>3.31</td><td>3.33</td><td>3.3</td><td>3.37</td><td>3.23</td><td>4.4</td></tr>
                            <tr><td>Engineering</td><td>3.11</td><td>2.55</td><td>3.97</td><td>4.79</td><td>6.03</td><td>3.97</td></tr>
                            <tr><td>Entertainment</td><td>2.89</td><td>2.51</td><td>3.4</td><td>8.07</td><td>9.04</td><td>7.28</td></tr>
                            <tr><td>Food_and_drink</td><td>0.81</td><td>0.72</td><td>0.93</td><td>3.7</td><td>5.5</td><td>2.79</td></tr>

                            <tr><td>Geography</td><td>2.81</td><td>4.92</td><td>1.97</td><td>8.41</td><td>14.52</td><td>5.92</td></tr>
                            <tr><td>Government</td><td>3.55</td><td>3.39</td><td>3.72</td><td>4.51</td><td>3.63</td><td>5.95</td></tr>
                            <tr><td>Health</td><td>2.73</td><td>3.23</td><td>2.36</td><td>5.69</td><td>5.19</td><td>6.3</td></tr>
                            <tr><td>History</td><td>5.24</td><td>5.79</td><td>4.78</td><td>10.28</td><td>8.52</td><td>12.97</td></tr>
                            <tr><td>Human_behavior</td><td>2.34</td><td>2.37</td><td>2.31</td><td>6.27</td><td>8.15</td><td>5.09</td></tr>

                            <tr><td>Humanities</td><td>2.76</td><td>3.09</td><td>2.49</td><td>1.62</td><td>4.44</td><td>0.99</td></tr>
                            <tr><td>Information</td><td>6</td><td>12</td><td>4</td><td>4.45</td><td>5.51</td><td>3.73</td></tr>
                            <tr><td>Internet</td><td>9.2</td><td>12.09</td><td>7.43</td><td>6.71</td><td>6.11</td><td>7.43</td></tr>
                            <tr><td>Knowledge</td><td>2.15</td><td>2.26</td><td>2.05</td><td>1.63</td><td>3.85</td><td>1.03</td></tr>
                            <tr><td>Language</td><td>7.53</td><td>8.02</td><td>7.1</td><td>4.18</td><td>8.93</td><td>2.73</td></tr>

                            <tr><td>Law</td><td>3.66</td><td>4.8</td><td>2.96</td><td>1.68</td><td>2.22</td><td>1.35</td></tr>
                            <tr><td>Life</td><td>2.72</td><td>4.55</td><td>1.94</td><td>5.84</td><td>20.59</td><td>3.4</td></tr>
                            <tr><td>Mathematics</td><td>9.82</td><td>11.27</td><td>8.7</td><td>10.3</td><td>15.91</td><td>7.61</td></tr>
                            <tr><td>Military</td><td>4.21</td><td>4.15</td><td>4.27</td><td>6.64</td><td>5.43</td><td>8.55</td></tr>
                            <tr><td>Nature</td><td>11.25</td><td>15.45</td><td>8.85</td><td>5.22</td><td>5.88</td><td>4.69</td></tr>

                            <tr><td>People</td><td>0.69</td><td>0.84</td><td>0.58</td><td>3.26</td><td>2.72</td><td>4.07</td></tr>
                            <tr><td>Philosophy</td><td>9.28</td><td>8.8</td><td>9.82</td><td>13.71</td><td>19.05</td><td>10.71</td></tr>
                            <tr><td>Politics</td><td>4.3</td><td>4.39</td><td>4.21</td><td>1.64</td><td>1.26</td><td>2.34</td></tr>
                            <tr><td>Religion</td><td>4.13</td><td>4.62</td><td>3.73</td><td>4.51</td><td>5.71</td><td>3.73</td></tr>
                            <tr><td>Science</td><td>10.33</td><td>12.09</td><td>9.02</td><td>1.49</td><td>8</td><td>0.82</td></tr>

                            <tr><td>Society</td><td>0.95</td><td>0.92</td><td>0.98</td><td>2.26</td><td>2.7</td><td>1.95</td></tr>
                            <tr><td>Sports</td><td>1.17</td><td>1.29</td><td>1.07</td><td>6.76</td><td>9.17</td><td>5.35</td></tr>
                            <tr><td>Technology</td><td>4.85</td><td>5.19</td><td>4.55</td><td>7.72</td><td>12.05</td><td>5.68</td></tr>
                            <tr><td>Universe</td><td>1.6</td><td>1.65</td><td>1.55</td><td>2.61</td><td>8.33</td><td>1.55</td></tr>
                        </table>



                    </div>
                </div>
            </div>



            
            <!-- / Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div id="submission" class="column is-four-fifths">
                    <h2 class="title is-3">Submission</h2>
                    <div class="content has-text-justified">
                        <p>
                            <!-- Our challenge aims to explore the domain transfer ability of LLM in document-level information extraction (DocIE) tasks in low-resource scenarios. Therefore, the dataset we provide contains document data from 34 different domains. Among them, we divide the datasets of 5 domains into training sets, the datasets of 2 domains into validation sets, and the datasets of the remaining multiple domains as test sets. The data of each domain contains 8-10 documents. -->
                            Our challenge seeks to investigate the domain transfer capabilities of large language models (LLMs) in document-level information extraction (DocIE) tasks, particularly in low-resource settings. To this end, we present a dataset that encompasses document data from many distinct domains. The dataset is divided as follows: <b>5 domains are designated for training, 2 domains for validation, and the remaining domains are allocated as test sets. Each domain dataset consists of 8 to 10 documents.</b>
                        </p>
                        <p>
                            <!-- Participants can use the training set we provide to train their own information extraction model and make predictions on the test set data we provide. <b>It is worth noting that we allow participants to use additional data for training.</b> Participants need to use the trained model to make predictions on the test set we provide, and organize the test results into the following format. -->
                            Participants may utilize the provided training set to develop their own information extraction models and make predictions on the test set. It is important to note that <b>the use of additional data for training is permitted.</b> Participants are required to apply their trained models to generate predictions on the test set and present the results in the specified format.
                        </p>
                        <p>
                            Participants need to successfully submit all the following files to be considered valid submissions:
                            <ol>
                                <li><b>Prediction result file (results.json)</b>: This file should contain the prediction results of the model on the test set, formatted according to the specified requirements.</li>
                                <li><b>Model weight file (*.ckpt, *.bin)</b>: Participants are required to provide the trained model weight file. The file should be uploaded to cloud storage, and the corresponding link must be recorded in the link.txt file.</li>
                                <li><b>Executable script file (*.py)</b>: An executable script file must be provided, which will be used in conjunction with the submitted model weights to verify the correctness of the provided results. The file should be uploaded to cloud storage, and the corresponding link must be recorded in the link.txt file.</li>
                            </ol>
                            The prediction result file(results.json) and the link file (link.txt) must be submitted via <a href="https://www.codabench.org/competitions/6641/">CodaBench</a>, which will be used to evaluate the submitted prediction results in real time. 
                        </p>
                        <p>
                             Please sumbit predicted results with a json files "results.json".
                        </p>
                        
                        <pre id="jsonShow">
{
    "Academic_disciplines_0": {
        "title": "Anthropocene_Working_Group",
        "entities": [
            {
                "mentions": [
                    "Industrial Revolution"
                ],
                "type": "event"
            },
            {
                "mentions": [
                    "HKW"
                ],
                "type": "organization"
            },
            ... ...
        ],
        "triples": [
            {
                "head": "Max Planck Institute",
                "relation": "part of",
                "tail": "Max Planck Society"
            },
            {
                "head": "Crawford Lake",
                "relation": "country",
                "tail": "Canada"
            },
            ... ... 
        ],
    },

    "Academic_disciplines_1": {
        ... ...
    },

    "Academic_disciplines_2": {
        ... ...
    },

    ... ...
}

<!-- [
{

    "id": 0,
    "domain": "DOMAIN_NAME",       # The domain of the text

    # Participants need to form the prediction results into the following structure
    # Task 1: Named Entity Recognition
    "entities_output": [
        {
            "mentions":["MENTION_1", "MENTION_2", ...],     # the mentions of the entity
            "type": "ENTITY_TYPE_1"                         # the type of the entity
        },
        {
            "mentions":["MENTION_1", "MENTION_2", ...],     # the mentions of the entity
            "type": "ENTITY_TYPE_2"                         # the type of the entity
        },
        ...
    ],


    # Task 2: Relation Extraction
    "triples_output": [
        {
            "head": "SUBJECT_1",         # The subject of the relation triplet
            "relation": "RELATION_1",       # The relation of the relation triplet
            "tail": "OBJECT_1"            # The object of the relation triplet

        },
        {
            "head": "SUBJECT_2",        # The subject of the relation triplet
            "relation": "RELATION_2",      # The relation of the relation triplet
            "tail": "OBJECT_2"           # The object of the relation triplet
        }
    ],

},

{
    "id": 1,
       ...
},
...
] -->
</pre>
                      
                    </div>
                </div>
            </div>


            <div class="columns is-centered has-text-centered">
                <div id="timeline" class="column is-four-fifths">
                    <h2 class="title is-3">Timeline</h2>
                    <div class="content has-text-justified">
                        <p>
                            Please note: The submission deadline is at 11:59 p.m. (<a herf="https://www.timeanddate.com/time/zones/aoe" style="color:red">Anywhere on Earth</a>) of the stated deadline date.
                        </p>
                        <table class="._table table-bordered table-striped" align="center">
                            <tbody align="center" valign="center">
                            <tr>
                              <td>Training data and participant instruction release for all shared tasks</td>
                              <td>February 10, 2025</td>
                            </tr>
                            <tr>
                              <td>Evaluation deadline for all shared tasks</td>
                              <td><s>March 30</s><font style="color: red;">Apr. 10, 2025</font></td>
                            </tr>
                            <tr>
                                <td>Notification of all shared tasks</td>
                                <td>April 5, 2025</td>
                            </tr>
                            <tr>
                                <td>Shared-task paper submission deadline</td>
                                <td>April 20, 2025</td>
                            </tr>
                            <tr>
                                <td>Acceptance notification of shared-task papers</td>
                                <td>April 30, 2025</td>
                            </tr>
                            <tr>
                                <td>Camera ready paper deadline</td>
                                <td>May 16, 2025</td>
                            </tr>
                        </tbody>
                        </table>
                    </div>
                </div>
            </div>

            <div class="columns is-centered has-text-centered">
                <div id="top-teams" class="column is-four-fifths">
                    <h2 class="title is-3">Top 2 Teams </h2>
                    <div class="content has-text-justified">
                        <ol>
                            <li style="color: red;">
                                <b>1st Place:</b> Team <b>qqpprun</b> - Kaifeng Wei, Chengfeng Qiu, Yuke Li, Haoqi Zhu - <i>Yidun AI Lab</i>
                            </li>
                            <li style="color: orange;">
                                <b>2nd Place:</b> Team <b>UIT-SHAMROCK</b> - Nguyen Pham Hoang Le, Nguyen Pham Hoang Le, Son T. Luu, Kiet Van Nguyen- <i>School of Computer Science and Technology, Fudan University</i>
                            </li>
                        </ol>
                    </div>
                </div>
            </div>
            
            
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Award of Top-ranking Participants</h2>
                    <div class="content has-text-justified">
                        <p>
                            Top-ranked participants in this competition will receive a certificate of achievement and will be recommended to write a technical paper for submission to the <a href="https://xllms.github.io">XLLM Workshop of ACL 2025</a>.
                        </p>
                    </div>
                </div>
            </div>   

            <!-- / Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Organizers</h2>
                    <div class="content has-text-justified">
                        
            <p>Zixia Jia (Beijing Institute for General Artificial Intelligence, Beijing, China)</p>
            <p>Zilong Zheng (Beijing Institute for General Artificial Intelligence, Beijing, China)</p>
            <p>Shuyi Zhang (Beijing Institute for General Artificial Intelligence, Beijing, China)</p>
            <p>Zhenbin Chen (Beijing Institute for General Artificial Intelligence, Beijing, China)</p>
                    </div>
                </div>
            </div>


            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">References</h2>
                    <div class="content has-text-justified">
                        <p>
                            [1] Yao, Yuan, et al. "DocRED: A large-scale document-level relation extraction dataset." arXiv preprint arXiv:1906.06127 (2019).
                        </p>
                        <p>
                            [2] Tan, Qingyu, et al. "Revisiting DocRED--Addressing the False Negative Problem in Relation Extraction." arXiv preprint arXiv:2205.12696 (2022).
                        </p>
                        <p>
                            [3] Li, Junpeng, Zixia Jia, and Zilong Zheng. "Semi-automatic data enhancement for document-level relation extraction with distant supervision from large language models." arXiv preprint arXiv:2311.07314 (2023).
                        </p>
                        <p>
                            [4] Gui, Honghao, et al. "Iepile: Unearthing large-scale schema-based information extraction corpus." arXiv preprint arXiv:2402.14710 (2024).
                        </p>
                        <p>
                            [5] Xue, Lilong, et al. "Autore: Document-level relation extraction with large language models." arXiv preprint arXiv:2403.14888 (2024).
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
</body>
</html>
